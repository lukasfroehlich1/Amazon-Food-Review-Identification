{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from support import query_data, unique_values\n",
    "from sklearn.manifold import TSNE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = sqlite3.connect(\"/data/amazon-fine-foods/amazon-fine-foods/database.sqlite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Fine Food Reviews, User Classification\n",
    "The objective of this project is to identify a user using a text review. This is a supervised classification problem. The goal is to produce a predictive model that when given a reasonable sample of the data is able to make accurate predictions about the author. One of the first issues to consider is that not every reviewer has made multiple reviews. This makes testing and training difficult as the testing set will match perfectly with the training set. This was accounted for by only considering users that have made at least a threshold number of reviews. \n",
    "Note: Do not actually run through this entire notebook. Many of the visualizations will not load. Reference \"2 - Exploration\" for the visualization code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1. Data Collection](#Data-Collection)\n",
    "\n",
    "[2. Visualization](#Visualization)\n",
    "\n",
    "[3. Model Fitting](#Model-Fitting)\n",
    "\n",
    "[4. Results](#Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "The data was obtained from <a href=\"https://www.kaggle.com/snap/amazon-fine-food-reviews\">kaggle</a>. The data consists of ~500,000 amazon fine food reviews. The dataset came as a csv as well as a sqlite database. For this project I chose to use the sqlite database to query the data. Overall the data was very clean and needed almost no cleaning to create a usable copy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           UserId                                               Text\n",
       "0  A3SGXH7AUHU8GW  I have bought several of the Vitality canned d...\n",
       "1  A1D87F6ZCVE5NK  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2   ABXLMWJIXXAIN  This is a confection that has been around a fe...\n",
       "3  A395BORC6FGVXV  If you are looking for the secret ingredient i...\n",
       "4  A1UQRSCLF8GW1T  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pd.read_sql_query(\"select UserId, Text \\\n",
    "                           from Reviews\", db)\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[\"Text\"][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was limited to only consider users that had a large volume of textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 users, 13218 reviews\n"
     ]
    }
   ],
   "source": [
    "review_min = 75\n",
    "reviews, rev_count, user_count = query_data(review_min, db)\n",
    "print(str(user_count), \"users,\", str(rev_count), \"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods Used\n",
    "Bag O' Words - TFIDF - term frequencyâ€“inverse document frequency \n",
    "\n",
    "<img src=\"td-idf-graphic.png\">\n",
    "\n",
    "<a href=\"http://filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html\">source</a>\n",
    "\n",
    "Ex: \"is\" is very common accross all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "The next step is to get a better understanding of the data and explore the effect of parameters from TFIDF vectorization. This is done by iterating over many different Tfidfvectorization parameters to try and visualize the effect they have. A smaller subset of the data is used to ensure to produce a clearer visualization. The following document on <a href=\"http://nbviewer.jupyter.org/urls/gist.githubusercontent.com/AlexanderFabisch/1a0c648de22eff4a2a3e/raw/59d5bc5ed8f8bfd9ff1f7faa749d1b095aa97d5a/t-SNE.ipynb\">TSNE</a> (t-Distributed Stochastic Neighbor Embedding) was the main source used for the visualization. The TSNE reduction process can take awhile when done on a large set. For the subset size used below the process takes around a minute or so depending on the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "reviews, rev_count, user_count = query_data(100, db)\n",
    "colors = unique_values(reviews)\n",
    "vectorizer = TfidfVectorizer()\n",
    "matrix = vectorizer.fit_transform(reviews[\"Text\"])\n",
    "X_transformed = TruncatedSVD(n_components=50).fit_transform(matrix)\n",
    "X_embedded = TSNE(n_components=2, perplexity=40, verbose=0).fit_transform(X_transformed)\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=colors, cmap=plt.cm.get_cmap('spectral', 10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"t-SNE visualization of TFIDF vecorization on Amazon Reviews (\"\n",
    "                                               + str(user_count) + \" users, \"\n",
    "                                               + str(rev_count) + \" reviews)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/fullviz.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at all the pretty colors! Unfortunately this data is difficult to process in its current state. Reducing the number of users as well as isolating parameters will yield a better understanding of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualization of TFIDF Parameters\n",
    "To start to get a better understanding of tfidf and the dataset a smaller set of the data will be analyzed. The methods used to visualize the data consist of a TFIDF vectorization of the reviews and then applying an SVD and then a TNSE reduction. SVD does not produce very good results and is mainly used for TSNE to process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By only looking at the 7 most active reviewrs we see that we can begin to very clearly see the groupings of uesrs' review pattern. Intersetingly it seems as if users can have multiple different writing styles as users seem to have multiple clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngram Range\n",
    "The Ngram range effects the size of ngram that is considered in the count stage. Therefore users who frequently use the same pattern of words will become more distingushed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'ngram_range':((1, 1), (1, 2), (2, 2), (4, 4))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "source": [
    "<img src=\"images/ngrams.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor\n",
    "A preprocessor function can be applied to words before they are counted and vectorized. A simple version of this is used to eliminate all punctuation that surrounds words. The expected result of this is to see more defined clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PUNCTUATION = '`~!@#$%^&*()_-+={[}]|\\:;\"<,>.?/}\\t\\n'\n",
    "def process(x):\n",
    "    \"\"\"\n",
    "    Basic Preprocessor to remove punctuation from words. \n",
    "    \"\"\"\n",
    "    return x.strip(PUNCTUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'preprocessor':(None, process)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/preprocessor.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum document frequency\n",
    "The TFIDF Vectorizer allows for a maximum document frequency to be set that only considers terms that occur below a particular document frequency. This eliminates common words and focuses on the more unique phrasing of each user.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'max_df':(0.25, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"images/maxdf.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model Fitting\n",
    "I initially considerd three models: RandomForest, MultinomialNB and BernoulliNB. Below we are constructing the pipeline to vectorize the text and then fit the RandomForestClassifier to it. The following process was extensively iterated upon which explains some of the reason why section below looks scattered. I initally started with most of the reasonable parameters in GridSearch and then slowly limited the values I search over until I no longer saw improvment. Once I found a value I liked I added it to the pipeline and focused on the remaining parameters. This process is important as running GridSearchCV on every possible parameter initially would have taken days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('rf', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are what can be iterated over using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['smooth_idf', 'lowercase', 'min_df', 'token_pattern', 'vocabulary', 'norm', 'analyzer', 'dtype', 'input', 'sublinear_tf', 'encoding', 'tokenizer', 'preprocessor', 'max_df', 'strip_accents', 'max_features', 'use_idf', 'binary', 'ngram_range', 'decode_error', 'stop_words'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TfidfVectorizer().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['warm_start', 'min_samples_leaf', 'random_state', 'max_leaf_nodes', 'class_weight', 'verbose', 'max_features', 'min_weight_fraction_leaf', 'bootstrap', 'n_estimators', 'max_depth', 'oob_score', 'criterion', 'n_jobs', 'min_samples_split'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using GridSearchCV iterations are able to be made over each of these parameters to find the configuration that yields the greatest classification. Along with RandomForestClassifier I also tried using MultinomialNB and BernoulliNB. These two did not have very good results therefore RandomForestClassifier was used in the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "With the three models previously tested, RandomForestClassifier produces the best results. GridSearchCV was heavily used to tweak the classifier to yield the greatest results. Those results are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PUNCTUATION = '`~!@#$%^&*()_-+={[}]|\\:;\"<,>.?/}\\t\\n'\n",
    "def process(x):\n",
    "    \"\"\"\n",
    "    Basic Preprocessor to remove punctuation from words. \n",
    "    \"\"\"\n",
    "    return x.strip(PUNCTUATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(preprocessor=process, norm='l1', ngram_range=(1, 2), max_df=1.0, max_features=18000)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=500)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = reviews[\"Text\"]\n",
    "y = reviews[\"UserId\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the data can take about a minute. I believe this is due to the number of estimators in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75219364599092287"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Areas\n",
    "Exploring Textual Data\n",
    "\n",
    "    -TFIDF\n",
    "    -TSNE and SVD\n",
    "    \n",
    "Extensively working with GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "Utilize RandomizedSearchCV initially to cut down on narrowing process.\n",
    "\n",
    "Start by breaking a piece of the data off initially to save for the end. Avoids variability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "What should the review minimum be set to? What guides that decision? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
